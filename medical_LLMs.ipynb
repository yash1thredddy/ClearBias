{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "vIp46rm4EZCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2VUkzMgEM0l"
      },
      "outputs": [],
      "source": [
        "import os, gc\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, quantize=False):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if quantize:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            bnb_8bit_compute_dtype=torch.float16,\n",
        "            bnb_8bit_use_double_quant=True,\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"cuda\"\n",
        "        )\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"cuda\"\n",
        "        )\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def generate_response(tokenizer, model, prompt, max_len=250):\n",
        "    text_generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "    formatted_prompt = f\"User: {prompt}\\nAssistant:\"\n",
        "    output = text_generator(formatted_prompt, max_length=max_len, do_sample=True)\n",
        "    response = output[0][\"generated_text\"][len(formatted_prompt):]\n",
        "    return response\n",
        "\n",
        "\n",
        "def free_mem(tokenizer, model):\n",
        "    model.to(\"cpu\")\n",
        "    del model\n",
        "    del tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def process_all_base_prompts(model_name, input_base_dir, output_base_dir):\n",
        "    tokenizer, model = load_model(model_name)\n",
        "    for file in os.listdir(input_base_dir):\n",
        "        if file.startswith('.') or not file.endswith('.csv'):\n",
        "            continue\n",
        "        file_path = os.path.join(input_base_dir, file)\n",
        "        csv_base = os.path.splitext(file)[0]\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        prompts = df[\"PROMPT\"].tolist()\n",
        "        bias_categories = df.get(\"BIAS CATEGORY\", \"\").tolist()\n",
        "        out_dir = os.path.join(output_base_dir, model_name.replace(\"/\", \"_\"))\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        results = []\n",
        "\n",
        "        for prompt, bias_category in zip(prompts, bias_categories):\n",
        "            response = generate_response(tokenizer, model, prompt)\n",
        "            result = {\n",
        "                \"MODEL\": model_name,\n",
        "                \"BIAS CATEGORY\": bias_category,\n",
        "                \"PROMPT\": prompt,\n",
        "                \"RESPONSE\": response,\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        if results:\n",
        "            output_file = os.path.join(out_dir, f\"{model_name.replace('/', '_')}_{csv_base}.csv\")\n",
        "            df_out = pd.DataFrame(results)\n",
        "            df_out.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
        "            print(f\"Saved {output_file} with {len(results)} responses\")\n",
        "\n",
        "    # clean vram\n",
        "    free_mem(tokenizer, model)"
      ],
      "metadata": {
        "id": "yh2Um9tAEfEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = [\"{your_models}\"]\n",
        "for model_name in model_names:\n",
        "    print(f\"Processing model: {model_name}\")\n",
        "    process_all_base_prompts(model_name, \"CLEAR-Bias/base_prompts\", \"results/base_prompts_finetuned\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "wx5O0nXSEzH6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}